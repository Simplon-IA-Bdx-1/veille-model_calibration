{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Model Calibration\n",
    "## ou l'amélioration de la distribution des erreurs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Rappel des metrics souvent utilisées\n",
    "- Principe de la Calibration de modèle\n",
    "- Class Calibration (CC)\n",
    "- Probabilistic Calibration (PC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calibration of prediction probabilities is a rescaling operation that is applied after the predictions have been made by a predictive model.\n",
    "\n",
    "There are two popular approaches to calibrating probabilities; they are the Platt Scaling and Isotonic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Nonlinear machine learning algorithms often predict uncalibrated class probabilities.\n",
    "Reliability diagrams can be used to diagnose the calibration of a model, and methods can be used to better calibrate predictions for a problem.\n",
    "\n",
    "Note, and this is really important: better calibrated probabilities may or may not lead to better class-based or probability-based predictions. It really depends on the specific metric used to evaluate predictions.\n",
    "\n",
    "In fact, some empirical results suggest that the algorithms that can benefit the more from calibrating predicted probabilities include SVMs, bagged decision trees, and random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Probability Calibration\n",
    "For instance, a well calibrated (binary) classifier should classify the samples such that among the samples to which it gave a predict_proba value close to 0.8, approximately 80% actually belong to the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# predict probabilities\n",
    "probs = model.predict_proba(testX)[:,1]\n",
    "# reliability diagram\n",
    "fop, mpv = calibration_curve(testy, probs, n_bins=10)\n",
    "# plot perfectly calibrated\n",
    "pyplot.plot([0, 1], [0, 1], linestyle='--')\n",
    "# plot model reliability\n",
    "pyplot.plot(mpv, fop, marker='.')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "doc \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\n",
    "\n",
    "https://towardsdatascience.com/calibration-techniques-of-machine-learning-models-d4f1a9c7a9cf\n",
    "\n",
    "https://machinelearningmastery.com/calibrated-classification-model-in-scikit-learn/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Diaporama",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
